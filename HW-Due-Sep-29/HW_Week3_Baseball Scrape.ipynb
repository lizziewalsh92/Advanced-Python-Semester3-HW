{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062adc48-7912-4ad3-8359-1c4f38527d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "134a4d13-00c5-497a-a96c-a0b4c8a5a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing all the stuff from the demo last week that i think i need\n",
    "\n",
    "import logging\n",
    "# import randint necessary library, this helps me use the snoozer i think?\n",
    "from random import randint, uniform\n",
    "## import logging, i think this is like console log but for python?\n",
    "import logging\n",
    "# need to use \"time\"'s sleep function\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eec7e558-366d-48fc-8422-c948fe5e3a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in /opt/anaconda3/lib/python3.12/site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from html5lib) (0.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## need to read html in python. says already installed. \n",
    "##does this mean if i pip install in one notebook, it will always be installed in future work? not sure..\n",
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e934e14f-b44d-4645-890d-0397e7ea49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making sure i don't look like a bot\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"           \n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                         \"Chrome/124.0.0.0 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "962c6f51-fdb5-4152-af12-974eaebe6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tables from url: https://www.baseball-reference.com/leagues/majors/2020-free-agents.shtml\n",
      "Table 1 scraped successfully\n",
      "Snoozing for 5.92 seconds before next table scrape\n",
      "Done scraping all URLs\n"
     ]
    }
   ],
   "source": [
    "## because there aren't numbered pages, we need to loop over the tables on one page instead of pages within one url\n",
    "## also need to handle errors differently, since we need to track table index errors instead of url errors\n",
    "## use \"finally:\" to snooze after each loop, just like the homework\n",
    "## experimenting with formatting the snoozer differently, since i don't need a million decimal points of snoozing time\n",
    "\n",
    "\n",
    "url = \"https://www.baseball-reference.com/leagues/majors/2020-free-agents.shtml\"\n",
    "df_list = []\n",
    "broken_links = []\n",
    "\n",
    "try: \n",
    "    tables = pd.read_html(url)## i needed to define the tables within the URL before beginnign the rest of code. \n",
    "                              ##*I USED CHAT GPT TO HELP ME DO THIS PART (figuring out code for tables vs full url pages)*\n",
    "    for i, df in enumerate(tables, start = 1): ## edited this line from demo, which *I USED CHAT GPT TO HELP ME DO*\n",
    "                                              ## I'm seeing that number and df are switched out as variables, and im working- \n",
    "                                              ##-with tables, not a page range\n",
    "        print(f\"Scraping tables from url: {url}\") ## i added this to make sure its scraping from the right place\n",
    "        try:\n",
    "            df[\"source_url\"]=url ## im still confused about what \"source_url\" is, but copied from demo\n",
    "            df_list.append(df)\n",
    "            print(f\"Table {i} scraped successfully\") ## added another print line to give me more info\n",
    "        except Exception as e:\n",
    "            print(f\"Scraping page{i}, url: {url}\")\n",
    "            broken_tables.append(i) ## again, \"i\" is subbed in for \"url\"\n",
    "        finally:\n",
    "            snoozer = uniform(2,7) ## switched up the snoozer time range as a test\n",
    "            print(f\"Snoozing for {snoozer:.2f} seconds before next table scrape\") ## shortening decimals\n",
    "            time.sleep(snoozer)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to scrape the page at all: {e}\") ## added this to tell me if the page itself is causing scrape issues (since im just scraping one page)\n",
    "                                                    ## *I USED CHAT GPT TO HELP ME DO THIS PART* \n",
    "\n",
    "print(\"Done scraping all URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bd3bfa1-c994-4b8b-a19a-e81972a64f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to my compuater\n"
     ]
    }
   ],
   "source": [
    "tables = pd.read_html(url) ## now i have to define what the \"tables\" variable is\n",
    "\n",
    "# make the df the only table on the site that i needed in this instance\n",
    "df = tables[0]\n",
    "\n",
    "# saving it to my comp\n",
    "df.to_csv(\"PythonSem3_Week3_HW_try2.csv\", index=False)\n",
    "\n",
    "print(\"Saved to my compuater\") ## tell me that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c81c9fa9-2689-44d5-9094-e36f24526e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 380 entries, 0 to 379\n",
      "Data columns (total 32 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Rk         380 non-null    int64  \n",
      " 1   Name       380 non-null    object \n",
      " 2   Date       380 non-null    object \n",
      " 3   To Team    380 non-null    object \n",
      " 4   From Team  380 non-null    object \n",
      " 5   Age        380 non-null    int64  \n",
      " 6   WAR3       365 non-null    float64\n",
      " 7   Yrs        380 non-null    int64  \n",
      " 8   G          348 non-null    float64\n",
      " 9   AB         348 non-null    float64\n",
      " 10  R          348 non-null    float64\n",
      " 11  H          348 non-null    float64\n",
      " 12  HR         348 non-null    float64\n",
      " 13  RBI        348 non-null    float64\n",
      " 14  SB         348 non-null    float64\n",
      " 15  BB         348 non-null    float64\n",
      " 16  BA         291 non-null    float64\n",
      " 17  OBP        292 non-null    float64\n",
      " 18  SLG        291 non-null    float64\n",
      " 19  OPS        291 non-null    float64\n",
      " 20  W          218 non-null    float64\n",
      " 21  L          218 non-null    float64\n",
      " 22  ERA        218 non-null    float64\n",
      " 23  WHIP       218 non-null    float64\n",
      " 24  G.1        218 non-null    float64\n",
      " 25  GS         218 non-null    float64\n",
      " 26  SV         218 non-null    float64\n",
      " 27  IP         218 non-null    float64\n",
      " 28  H.1        218 non-null    float64\n",
      " 29  HR.1       218 non-null    float64\n",
      " 30  BB.1       218 non-null    float64\n",
      " 31  SO         218 non-null    float64\n",
      "dtypes: float64(25), int64(3), object(4)\n",
      "memory usage: 95.1+ KB\n"
     ]
    }
   ],
   "source": [
    "## making sure my scrape worked\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fc8cfd5-53fb-47eb-a2ce-837623fb3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65031c-bb9a-418e-bc7b-e8556bc8d00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
